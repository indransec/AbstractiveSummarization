{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e3e82e82",
      "metadata": {
        "id": "e3e82e82",
        "papermill": {
          "duration": 0.010709,
          "end_time": "2023-02-05T19:58:40.557366",
          "exception": false,
          "start_time": "2023-02-05T19:58:40.546657",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivanandroy/T5-Finetuning-PyTorch/blob/main/notebook/T5_Fine_tuning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02492a0c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T19:58:40.633617Z",
          "iopub.status.busy": "2023-02-05T19:58:40.633310Z",
          "iopub.status.idle": "2023-02-05T19:59:11.434273Z",
          "shell.execute_reply": "2023-02-05T19:59:11.433043Z"
        },
        "id": "02492a0c",
        "outputId": "20118fde-a280-43fc-baa7-271cff13785d",
        "papermill": {
          "duration": 30.814134,
          "end_time": "2023-02-05T19:59:11.437115",
          "exception": false,
          "start_time": "2023-02-05T19:58:40.622981",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\r\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\r\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\r\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0.0)\r\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\r\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\r\n",
            "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\r\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\r\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\r\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\r\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0mRequirement already satisfied: rich[jupyter] in /opt/conda/lib/python3.7/site-packages (12.6.0)\r\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from rich[jupyter]) (4.1.1)\r\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich[jupyter]) (2.12.0)\r\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.7/site-packages (from rich[jupyter]) (0.9.1)\r\n",
            "Requirement already satisfied: ipywidgets<8.0.0,>=7.5.1 in /opt/conda/lib/python3.7/site-packages (from rich[jupyter]) (7.7.1)\r\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (5.3.0)\r\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.2.0)\r\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.1.1)\r\n",
            "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (7.33.0)\r\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (3.6.1)\r\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (6.15.0)\r\n",
            "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (6.1)\r\n",
            "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.5.5)\r\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.1.3)\r\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (5.9.1)\r\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (7.3.4)\r\n",
            "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.6.0)\r\n",
            "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (23.2.0)\r\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (23.0)\r\n",
            "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.7.5)\r\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (3.0.30)\r\n",
            "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (59.8.0)\r\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.18.1)\r\n",
            "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (5.1.1)\r\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (4.8.0)\r\n",
            "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.2.0)\r\n",
            "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (6.4.12)\r\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.8.3)\r\n",
            "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.4)\r\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (2.8.2)\r\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.7/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (4.10.0)\r\n",
            "Requirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (5.4.0)\r\n",
            "Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (6.4.5)\r\n",
            "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.14.1)\r\n",
            "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (21.3.0)\r\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (3.1.2)\r\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.8.0)\r\n",
            "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.15.0)\r\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.7.0)\r\n",
            "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.2.5)\r\n",
            "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (5.0.1)\r\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (4.11.1)\r\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.5.13)\r\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (2.1.2)\r\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.8.4)\r\n",
            "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.6.0)\r\n",
            "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.7.1)\r\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.5.0)\r\n",
            "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.2.2)\r\n",
            "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.7/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (4.6.1)\r\n",
            "Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.7/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (2.15.3)\r\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.15.0)\r\n",
            "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (21.2.0)\r\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (6.0.0)\r\n",
            "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (21.4.0)\r\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (5.10.2)\r\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.18.1)\r\n",
            "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (1.15.0)\r\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (2.3.1)\r\n",
            "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (0.5.1)\r\n",
            "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (2.21)\r\n",
            "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<8.0.0,>=7.5.1->rich[jupyter]) (3.8.0)\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install rich[jupyter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85207f2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T19:59:11.460118Z",
          "iopub.status.busy": "2023-02-05T19:59:11.459790Z",
          "iopub.status.idle": "2023-02-05T20:00:26.514227Z",
          "shell.execute_reply": "2023-02-05T20:00:26.512941Z"
        },
        "papermill": {
          "duration": 75.068908,
          "end_time": "2023-02-05T20:00:26.517014",
          "exception": false,
          "start_time": "2023-02-05T19:59:11.448106",
          "status": "completed"
        },
        "tags": [],
        "id": "f85207f2",
        "outputId": "0da92330-3396-4aab-fb5f-eda2eaa58651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n",
            "\r\n",
            "\r\n",
            "==> WARNING: A newer version of conda exists. <==\r\n",
            "  current version: 22.9.0\r\n",
            "  latest version: 22.11.1\r\n",
            "\r\n",
            "Please update conda by running\r\n",
            "\r\n",
            "    $ conda update -n base -c conda-forge conda\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "## Package Plan ##\r\n",
            "\r\n",
            "  environment location: /opt/conda\r\n",
            "\r\n",
            "  added / updated specs:\r\n",
            "    - gdown\r\n",
            "\r\n",
            "\r\n",
            "The following packages will be downloaded:\r\n",
            "\r\n",
            "    package                    |            build\r\n",
            "    ---------------------------|-----------------\r\n",
            "    filelock-3.9.0             |     pyhd8ed1ab_0          13 KB  conda-forge\r\n",
            "    gdown-4.6.0                |     pyhd8ed1ab_0          18 KB  conda-forge\r\n",
            "    ------------------------------------------------------------\r\n",
            "                                           Total:          31 KB\r\n",
            "\r\n",
            "The following NEW packages will be INSTALLED:\r\n",
            "\r\n",
            "  filelock           conda-forge/noarch::filelock-3.9.0-pyhd8ed1ab_0 None\r\n",
            "  gdown              conda-forge/noarch::gdown-4.6.0-pyhd8ed1ab_0 None\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Downloading and Extracting Packages\r\n",
            "gdown-4.6.0          | 18 KB     | ##################################### | 100% \r\n",
            "filelock-3.9.0       | 13 KB     | ##################################### | 100% \r\n",
            "Preparing transaction: - \b\bdone\r\n",
            "Verifying transaction: | \b\bdone\r\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
            "Retrieving notices: ...working... done\r\n"
          ]
        }
      ],
      "source": [
        "!conda install -y gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355277e9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:00:43.916622Z",
          "iopub.status.busy": "2023-02-05T20:00:43.915985Z",
          "iopub.status.idle": "2023-02-05T20:00:55.001788Z",
          "shell.execute_reply": "2023-02-05T20:00:55.000577Z"
        },
        "papermill": {
          "duration": 11.120571,
          "end_time": "2023-02-05T20:00:55.004170",
          "exception": false,
          "start_time": "2023-02-05T20:00:43.883599",
          "status": "completed"
        },
        "tags": [],
        "id": "355277e9",
        "outputId": "50899b48-6773-4bbc-ef1e-da63d8b44d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/working\r\n",
            "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.8.1)\r\n",
            "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.64.0)\r\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\r\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.11.10)\r\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (8.1.3)\r\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk) (6.0.0)\r\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.0)\r\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.1.1)\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pwd\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76e12a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:00:55.071776Z",
          "iopub.status.busy": "2023-02-05T20:00:55.071403Z",
          "iopub.status.idle": "2023-02-05T20:00:58.578758Z",
          "shell.execute_reply": "2023-02-05T20:00:58.577595Z"
        },
        "id": "b76e12a4",
        "papermill": {
          "duration": 3.543403,
          "end_time": "2023-02-05T20:00:58.581388",
          "exception": false,
          "start_time": "2023-02-05T20:00:55.037985",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console=Console(record=True)\n",
        "\n",
        "def display_df(df):\n",
        "  \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "  console=Console()\n",
        "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
        "\n",
        "  for i, row in enumerate(df.values.tolist()):\n",
        "    table.add_row(row[0], row[1])\n",
        "\n",
        "  console.print(table)\n",
        "\n",
        "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
        "                        Column(\"Steps\", justify=\"center\"),\n",
        "                        Column(\"Loss\", justify=\"center\"), \n",
        "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e846a452",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:00:58.647204Z",
          "iopub.status.busy": "2023-02-05T20:00:58.646890Z",
          "iopub.status.idle": "2023-02-05T20:01:11.177315Z",
          "shell.execute_reply": "2023-02-05T20:01:11.176334Z"
        },
        "id": "e846a452",
        "papermill": {
          "duration": 12.566053,
          "end_time": "2023-02-05T20:01:11.179854",
          "exception": false,
          "start_time": "2023-02-05T20:00:58.613801",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainnews = pd.read_csv(\"./Book1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8675889",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:11.247473Z",
          "iopub.status.busy": "2023-02-05T20:01:11.247124Z",
          "iopub.status.idle": "2023-02-05T20:01:11.265271Z",
          "shell.execute_reply": "2023-02-05T20:01:11.264246Z"
        },
        "papermill": {
          "duration": 0.054886,
          "end_time": "2023-02-05T20:01:11.267884",
          "exception": false,
          "start_time": "2023-02-05T20:01:11.212998",
          "status": "completed"
        },
        "tags": [],
        "id": "e8675889",
        "outputId": "01e277c4-b0ff-4c3f-bf03-8fe8abab8067"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
              "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ralph Mata was an internal affairs lieutenant ...</td>\n",
              "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A drunk driver who killed a young woman in a h...</td>\n",
              "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With a breezy sweep of his pen President Vladi...</td>\n",
              "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fleetwood are the only team still to have a 10...</td>\n",
              "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  \\\n",
              "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
              "1  Ralph Mata was an internal affairs lieutenant ...   \n",
              "2  A drunk driver who killed a young woman in a h...   \n",
              "3  With a breezy sweep of his pen President Vladi...   \n",
              "4  Fleetwood are the only team still to have a 10...   \n",
              "\n",
              "                                               Short  \n",
              "0  Bishop John Folda, of North Dakota, is taking ...  \n",
              "1  Criminal complaint: Cop used his role to help ...  \n",
              "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
              "3  Nina dos Santos says Europe must be ready to a...  \n",
              "4  Fleetwood top of League One after 2-0 win at S...  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainnews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4deba6f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:11.334316Z",
          "iopub.status.busy": "2023-02-05T20:01:11.334043Z",
          "iopub.status.idle": "2023-02-05T20:01:11.338140Z",
          "shell.execute_reply": "2023-02-05T20:01:11.337127Z"
        },
        "papermill": {
          "duration": 0.039695,
          "end_time": "2023-02-05T20:01:11.340506",
          "exception": false,
          "start_time": "2023-02-05T20:01:11.300811",
          "status": "completed"
        },
        "tags": [],
        "id": "a4deba6f"
      },
      "outputs": [],
      "source": [
        "#trainnews.drop(\"Unnamed: 2\", axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f0e41f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:11.409958Z",
          "iopub.status.busy": "2023-02-05T20:01:11.408948Z",
          "iopub.status.idle": "2023-02-05T20:01:11.414253Z",
          "shell.execute_reply": "2023-02-05T20:01:11.413283Z"
        },
        "id": "d9f0e41f",
        "papermill": {
          "duration": 0.043353,
          "end_time": "2023-02-05T20:01:11.416323",
          "exception": false,
          "start_time": "2023-02-05T20:01:11.372970",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainnews=trainnews[:100000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a306b6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:11.483836Z",
          "iopub.status.busy": "2023-02-05T20:01:11.483560Z",
          "iopub.status.idle": "2023-02-05T20:01:12.061360Z",
          "shell.execute_reply": "2023-02-05T20:01:12.060395Z"
        },
        "id": "d9a306b6",
        "papermill": {
          "duration": 0.613878,
          "end_time": "2023-02-05T20:01:12.063756",
          "exception": false,
          "start_time": "2023-02-05T20:01:11.449878",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "valnews = pd.read_csv(\"./validation.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1f4733",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:12.276152Z",
          "iopub.status.busy": "2023-02-05T20:01:12.275509Z",
          "iopub.status.idle": "2023-02-05T20:01:12.280084Z",
          "shell.execute_reply": "2023-02-05T20:01:12.279187Z"
        },
        "id": "4b1f4733",
        "papermill": {
          "duration": 0.03934,
          "end_time": "2023-02-05T20:01:12.281998",
          "exception": false,
          "start_time": "2023-02-05T20:01:12.242658",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainnews.columns=['article','highlights']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018becfd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:12.351035Z",
          "iopub.status.busy": "2023-02-05T20:01:12.350129Z",
          "iopub.status.idle": "2023-02-05T20:01:12.365503Z",
          "shell.execute_reply": "2023-02-05T20:01:12.364289Z"
        },
        "id": "018becfd",
        "outputId": "a9c0dadf-fcc0-460a-cb1d-13f2cf457c48",
        "papermill": {
          "duration": 0.052001,
          "end_time": "2023-02-05T20:01:12.367817",
          "exception": false,
          "start_time": "2023-02-05T20:01:12.315816",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56032</th>\n",
              "      <td>Elizabeth Edwards tormented herself by using v...</td>\n",
              "      <td>Tell-all memoir, 'What Really Happened: John E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35773</th>\n",
              "      <td>An elderly woman driver escaped serious injury...</td>\n",
              "      <td>The female driver, who is in her 80s, escaped ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6732</th>\n",
              "      <td>The highest paid female business executive in ...</td>\n",
              "      <td>Martine Rothblatt, 60, was married father-of-f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34425</th>\n",
              "      <td>By . Mail Foreign Service . PUBLISHED: . 16:02...</td>\n",
              "      <td>Palestinians: Move casts doubt on Israel's 'si...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90282</th>\n",
              "      <td>England new boy Nathaniel Clyne looked at home...</td>\n",
              "      <td>Wayne Rooney leads England training ahead of E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62146</th>\n",
              "      <td>This week's bitter budget showdown in Wisconsi...</td>\n",
              "      <td>Governor can't get a quorum in Wisconsin Senat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68149</th>\n",
              "      <td>Bronze Age Brits were wearing bling around 4,0...</td>\n",
              "      <td>The 4,000-year-old necklace was found in an ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8584</th>\n",
              "      <td>By . Matt Chorley, Mailonline Political Editor...</td>\n",
              "      <td>Catalogue of equipment stolen from April to Se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75505</th>\n",
              "      <td>Rangers caretaker boss Kenny McDowall has sens...</td>\n",
              "      <td>Rangers have loaned five players from Newcastl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73829</th>\n",
              "      <td>Hundreds of dolphins are seen diving through t...</td>\n",
              "      <td>The dolphins migrate from Mossel Bay, South Af...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 article  \\\n",
              "56032  Elizabeth Edwards tormented herself by using v...   \n",
              "35773  An elderly woman driver escaped serious injury...   \n",
              "6732   The highest paid female business executive in ...   \n",
              "34425  By . Mail Foreign Service . PUBLISHED: . 16:02...   \n",
              "90282  England new boy Nathaniel Clyne looked at home...   \n",
              "62146  This week's bitter budget showdown in Wisconsi...   \n",
              "68149  Bronze Age Brits were wearing bling around 4,0...   \n",
              "8584   By . Matt Chorley, Mailonline Political Editor...   \n",
              "75505  Rangers caretaker boss Kenny McDowall has sens...   \n",
              "73829  Hundreds of dolphins are seen diving through t...   \n",
              "\n",
              "                                              highlights  \n",
              "56032  Tell-all memoir, 'What Really Happened: John E...  \n",
              "35773  The female driver, who is in her 80s, escaped ...  \n",
              "6732   Martine Rothblatt, 60, was married father-of-f...  \n",
              "34425  Palestinians: Move casts doubt on Israel's 'si...  \n",
              "90282  Wayne Rooney leads England training ahead of E...  \n",
              "62146  Governor can't get a quorum in Wisconsin Senat...  \n",
              "68149  The 4,000-year-old necklace was found in an ar...  \n",
              "8584   Catalogue of equipment stolen from April to Se...  \n",
              "75505  Rangers have loaned five players from Newcastl...  \n",
              "73829  The dolphins migrate from Mossel Bay, South Af...  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainnews.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab91eb1d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:12.442052Z",
          "iopub.status.busy": "2023-02-05T20:01:12.441154Z",
          "iopub.status.idle": "2023-02-05T20:01:12.453527Z",
          "shell.execute_reply": "2023-02-05T20:01:12.452471Z"
        },
        "id": "ab91eb1d",
        "outputId": "8f88e689-9e8f-44ea-9c5e-5b625be03b0a",
        "papermill": {
          "duration": 0.051872,
          "end_time": "2023-02-05T20:01:12.455623",
          "exception": false,
          "start_time": "2023-02-05T20:01:12.403751",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8238</th>\n",
              "      <td>a02c833b59349282e09212a2e6c6ecd308ea3e46</td>\n",
              "      <td>No-one enjoys filing their tax return, but pay...</td>\n",
              "      <td>Tax receipt written in Greek on pottery is dat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9274</th>\n",
              "      <td>c02bf0991fa612825581857f246c29a7b5820083</td>\n",
              "      <td>(CNN)A Georgia police chief who said he accide...</td>\n",
              "      <td>William McCollom says he accidentally shot wif...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12860</th>\n",
              "      <td>44e182d721c6d78267b3e288e886b4efd6abc693</td>\n",
              "      <td>Cancel your plans to journey through the centr...</td>\n",
              "      <td>Originally it was thought the journey from one...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9971</th>\n",
              "      <td>dfe8ae0b69179fb9ebe7a81a24ba0baded2e8323</td>\n",
              "      <td>Three police officers have been questioned as ...</td>\n",
              "      <td>EXCLUSIVE: Poppi Worthington, from Barrow-in-F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>762292391425a10d9dec2fdd077fe2aaf02021fa</td>\n",
              "      <td>A north Texas woman is outraged with her five-...</td>\n",
              "      <td>Gabriel Muntu, 5, was supposed to remain at sc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5486</th>\n",
              "      <td>6744e427592c9a29a68051c861b0b26139ade3d2</td>\n",
              "      <td>Newcastle have announced record profits of £18...</td>\n",
              "      <td>Newcastle have announced record profits of £18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>fa1a65db1127880f704ff189754e1123d164ba7f</td>\n",
              "      <td>(CNN)Once again the global community waits to ...</td>\n",
              "      <td>Authors: Iran's people want a deal in hopes of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3029</th>\n",
              "      <td>fbd22d5f685ec8d006201c41e36fd092e26d6bd3</td>\n",
              "      <td>(CNN)There's a steady stream of blood flowing ...</td>\n",
              "      <td>The documentary \"Dawg Fight\" shows men fightin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10363</th>\n",
              "      <td>581c1a4bd166e8571fec9799d2e47b72024c54f5</td>\n",
              "      <td>If you want the memory of your loved ones or f...</td>\n",
              "      <td>The Bios Urn has been developed by Barcelona-b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6552</th>\n",
              "      <td>b9550576c750440c1cbf2a6a3e6d706e71739eea</td>\n",
              "      <td>This is the moment an abandoned dog is rescued...</td>\n",
              "      <td>Video footage shows animal rescue worker Eldad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             id  \\\n",
              "8238   a02c833b59349282e09212a2e6c6ecd308ea3e46   \n",
              "9274   c02bf0991fa612825581857f246c29a7b5820083   \n",
              "12860  44e182d721c6d78267b3e288e886b4efd6abc693   \n",
              "9971   dfe8ae0b69179fb9ebe7a81a24ba0baded2e8323   \n",
              "889    762292391425a10d9dec2fdd077fe2aaf02021fa   \n",
              "5486   6744e427592c9a29a68051c861b0b26139ade3d2   \n",
              "771    fa1a65db1127880f704ff189754e1123d164ba7f   \n",
              "3029   fbd22d5f685ec8d006201c41e36fd092e26d6bd3   \n",
              "10363  581c1a4bd166e8571fec9799d2e47b72024c54f5   \n",
              "6552   b9550576c750440c1cbf2a6a3e6d706e71739eea   \n",
              "\n",
              "                                                 article  \\\n",
              "8238   No-one enjoys filing their tax return, but pay...   \n",
              "9274   (CNN)A Georgia police chief who said he accide...   \n",
              "12860  Cancel your plans to journey through the centr...   \n",
              "9971   Three police officers have been questioned as ...   \n",
              "889    A north Texas woman is outraged with her five-...   \n",
              "5486   Newcastle have announced record profits of £18...   \n",
              "771    (CNN)Once again the global community waits to ...   \n",
              "3029   (CNN)There's a steady stream of blood flowing ...   \n",
              "10363  If you want the memory of your loved ones or f...   \n",
              "6552   This is the moment an abandoned dog is rescued...   \n",
              "\n",
              "                                              highlights  \n",
              "8238   Tax receipt written in Greek on pottery is dat...  \n",
              "9274   William McCollom says he accidentally shot wif...  \n",
              "12860  Originally it was thought the journey from one...  \n",
              "9971   EXCLUSIVE: Poppi Worthington, from Barrow-in-F...  \n",
              "889    Gabriel Muntu, 5, was supposed to remain at sc...  \n",
              "5486   Newcastle have announced record profits of £18...  \n",
              "771    Authors: Iran's people want a deal in hopes of...  \n",
              "3029   The documentary \"Dawg Fight\" shows men fightin...  \n",
              "10363  The Bios Urn has been developed by Barcelona-b...  \n",
              "6552   Video footage shows animal rescue worker Eldad...  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valnews.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddab64a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:12.528729Z",
          "iopub.status.busy": "2023-02-05T20:01:12.528405Z",
          "iopub.status.idle": "2023-02-05T20:01:12.538918Z",
          "shell.execute_reply": "2023-02-05T20:01:12.537926Z"
        },
        "id": "9ddab64a",
        "papermill": {
          "duration": 0.049964,
          "end_time": "2023-02-05T20:01:12.541249",
          "exception": false,
          "start_time": "2023-02-05T20:01:12.491285",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "valnews.drop(\"id\",axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "696f1a4c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:12.615947Z",
          "iopub.status.busy": "2023-02-05T20:01:12.614220Z",
          "iopub.status.idle": "2023-02-05T20:01:12.626584Z",
          "shell.execute_reply": "2023-02-05T20:01:12.625426Z"
        },
        "id": "696f1a4c",
        "outputId": "5c90e25b-9e0c-4ce6-f0e5-57f1f7f6e846",
        "papermill": {
          "duration": 0.054231,
          "end_time": "2023-02-05T20:01:12.628818",
          "exception": false,
          "start_time": "2023-02-05T20:01:12.574587",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4553</th>\n",
              "      <td>Washington (CNN)House Speaker John Boehner sai...</td>\n",
              "      <td>House Speaker says his problem with Bergdahl c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5315</th>\n",
              "      <td>England head coach Peter Moores, who should su...</td>\n",
              "      <td>Charles Sale: Peter Moores should be sacked af...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12702</th>\n",
              "      <td>An airline has paid an undisclosed amount to m...</td>\n",
              "      <td>Financial details of the settlements remain co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7298</th>\n",
              "      <td>(CNN)The \"Fifty Shades\" series will have to ha...</td>\n",
              "      <td>Sam Taylor-Johnson bows out of the \"Fifty Shad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7200</th>\n",
              "      <td>April 1, 2015 . A trip around the world starts...</td>\n",
              "      <td>This page includes the show Transcript .\\nUse ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8897</th>\n",
              "      <td>A couple are facing jail after they blackmaile...</td>\n",
              "      <td>Chauffeur Awad Abdulbagy drove family's photog...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6979</th>\n",
              "      <td>It was once a symbol of U.S. supremacy before ...</td>\n",
              "      <td>Former U.S. embassy in Tehran is a chilling mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8270</th>\n",
              "      <td>A convicted robber has sparked outrage after p...</td>\n",
              "      <td>Adam Ali is currently serving a seven year sen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1314</th>\n",
              "      <td>Ryanair has backtracked on plans to develop a ...</td>\n",
              "      <td>Ryanair had announced plans for a service betw...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11585</th>\n",
              "      <td>Imagine playing a game of Pictionary with a gr...</td>\n",
              "      <td>Suki Kim, a Korean-American journalist, taught...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 article  \\\n",
              "4553   Washington (CNN)House Speaker John Boehner sai...   \n",
              "5315   England head coach Peter Moores, who should su...   \n",
              "12702  An airline has paid an undisclosed amount to m...   \n",
              "7298   (CNN)The \"Fifty Shades\" series will have to ha...   \n",
              "7200   April 1, 2015 . A trip around the world starts...   \n",
              "8897   A couple are facing jail after they blackmaile...   \n",
              "6979   It was once a symbol of U.S. supremacy before ...   \n",
              "8270   A convicted robber has sparked outrage after p...   \n",
              "1314   Ryanair has backtracked on plans to develop a ...   \n",
              "11585  Imagine playing a game of Pictionary with a gr...   \n",
              "\n",
              "                                              highlights  \n",
              "4553   House Speaker says his problem with Bergdahl c...  \n",
              "5315   Charles Sale: Peter Moores should be sacked af...  \n",
              "12702  Financial details of the settlements remain co...  \n",
              "7298   Sam Taylor-Johnson bows out of the \"Fifty Shad...  \n",
              "7200   This page includes the show Transcript .\\nUse ...  \n",
              "8897   Chauffeur Awad Abdulbagy drove family's photog...  \n",
              "6979   Former U.S. embassy in Tehran is a chilling mu...  \n",
              "8270   Adam Ali is currently serving a seven year sen...  \n",
              "1314   Ryanair had announced plans for a service betw...  \n",
              "11585  Suki Kim, a Korean-American journalist, taught...  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valnews.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429a7a61",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:12.701722Z",
          "iopub.status.busy": "2023-02-05T20:01:12.700823Z",
          "iopub.status.idle": "2023-02-05T20:01:13.151232Z",
          "shell.execute_reply": "2023-02-05T20:01:13.150117Z"
        },
        "id": "429a7a61",
        "papermill": {
          "duration": 0.489056,
          "end_time": "2023-02-05T20:01:13.154510",
          "exception": false,
          "start_time": "2023-02-05T20:01:12.665454",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainnews[\"article\"] = \"summarize: \"+trainnews[\"article\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa7ee35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.232898Z",
          "iopub.status.busy": "2023-02-05T20:01:13.231968Z",
          "iopub.status.idle": "2023-02-05T20:01:13.241641Z",
          "shell.execute_reply": "2023-02-05T20:01:13.240662Z"
        },
        "id": "bfa7ee35",
        "outputId": "08ae4573-0381-4098-aca5-fbf9c5e242cb",
        "papermill": {
          "duration": 0.045245,
          "end_time": "2023-02-05T20:01:13.243983",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.198738",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>summarize: By . Associated Press . PUBLISHED: ...</td>\n",
              "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>summarize: Ralph Mata was an internal affairs ...</td>\n",
              "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>summarize: A drunk driver who killed a young w...</td>\n",
              "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>summarize: With a breezy sweep of his pen Pres...</td>\n",
              "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>summarize: Fleetwood are the only team still t...</td>\n",
              "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             article  \\\n",
              "0  summarize: By . Associated Press . PUBLISHED: ...   \n",
              "1  summarize: Ralph Mata was an internal affairs ...   \n",
              "2  summarize: A drunk driver who killed a young w...   \n",
              "3  summarize: With a breezy sweep of his pen Pres...   \n",
              "4  summarize: Fleetwood are the only team still t...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Bishop John Folda, of North Dakota, is taking ...  \n",
              "1  Criminal complaint: Cop used his role to help ...  \n",
              "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
              "3  Nina dos Santos says Europe must be ready to a...  \n",
              "4  Fleetwood top of League One after 2-0 win at S...  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainnews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fafdfab8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.312462Z",
          "iopub.status.busy": "2023-02-05T20:01:13.311675Z",
          "iopub.status.idle": "2023-02-05T20:01:13.338283Z",
          "shell.execute_reply": "2023-02-05T20:01:13.337360Z"
        },
        "id": "fafdfab8",
        "papermill": {
          "duration": 0.062879,
          "end_time": "2023-02-05T20:01:13.340882",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.278003",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "valnews[\"article\"] = \"summarize: \"+valnews[\"article\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e7386e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.413832Z",
          "iopub.status.busy": "2023-02-05T20:01:13.412962Z",
          "iopub.status.idle": "2023-02-05T20:01:13.423141Z",
          "shell.execute_reply": "2023-02-05T20:01:13.422133Z"
        },
        "id": "7e7386e8",
        "outputId": "533cd429-999f-45ee-821a-f0537bf44119",
        "papermill": {
          "duration": 0.049474,
          "end_time": "2023-02-05T20:01:13.425266",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.375792",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>summarize: Sally Forrest, an actress-dancer wh...</td>\n",
              "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>summarize: A middle-school teacher in China ha...</td>\n",
              "      <td>Works include pictures of Presidential Palace ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>summarize: A man convicted of killing the fath...</td>\n",
              "      <td>Iftekhar Murtaza, 29, was convicted a year ago...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>summarize: Avid rugby fan Prince Harry could b...</td>\n",
              "      <td>Prince Harry in attendance for England's crunc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>summarize: A Triple M Radio producer has been ...</td>\n",
              "      <td>Nick Slater's colleagues uploaded a picture to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             article  \\\n",
              "0  summarize: Sally Forrest, an actress-dancer wh...   \n",
              "1  summarize: A middle-school teacher in China ha...   \n",
              "2  summarize: A man convicted of killing the fath...   \n",
              "3  summarize: Avid rugby fan Prince Harry could b...   \n",
              "4  summarize: A Triple M Radio producer has been ...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Sally Forrest, an actress-dancer who graced th...  \n",
              "1  Works include pictures of Presidential Palace ...  \n",
              "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
              "3  Prince Harry in attendance for England's crunc...  \n",
              "4  Nick Slater's colleagues uploaded a picture to...  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valnews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655034f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.492706Z",
          "iopub.status.busy": "2023-02-05T20:01:13.491784Z",
          "iopub.status.idle": "2023-02-05T20:01:13.573286Z",
          "shell.execute_reply": "2023-02-05T20:01:13.572236Z"
        },
        "id": "655034f6",
        "papermill": {
          "duration": 0.117721,
          "end_time": "2023-02-05T20:01:13.575881",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.458160",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8165d9f8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.644338Z",
          "iopub.status.busy": "2023-02-05T20:01:13.644030Z",
          "iopub.status.idle": "2023-02-05T20:01:13.650127Z",
          "shell.execute_reply": "2023-02-05T20:01:13.649145Z"
        },
        "id": "8165d9f8",
        "outputId": "2c1b2297-ff2c-432c-ccc8-44a7f867b633",
        "papermill": {
          "duration": 0.042189,
          "end_time": "2023-02-05T20:01:13.652124",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.609935",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5821a299",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.721144Z",
          "iopub.status.busy": "2023-02-05T20:01:13.720858Z",
          "iopub.status.idle": "2023-02-05T20:01:13.731363Z",
          "shell.execute_reply": "2023-02-05T20:01:13.730535Z"
        },
        "id": "5821a299",
        "papermill": {
          "duration": 0.04749,
          "end_time": "2023-02-05T20:01:13.733334",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.685844",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class YourDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    #cleaning data so as to ensure data is in string type\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2974fada",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.806138Z",
          "iopub.status.busy": "2023-02-05T20:01:13.805366Z",
          "iopub.status.idle": "2023-02-05T20:01:13.813905Z",
          "shell.execute_reply": "2023-02-05T20:01:13.812812Z"
        },
        "id": "2974fada",
        "papermill": {
          "duration": 0.047257,
          "end_time": "2023-02-05T20:01:13.815991",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.768734",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to be called for training with the parameters passed from main function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "  for _,data in enumerate(loader, 0):\n",
        "    y = data['target_ids'].to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if _%1000==0:\n",
        "      training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "      console.print(training_logger)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af87479",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:13.887132Z",
          "iopub.status.busy": "2023-02-05T20:01:13.886856Z",
          "iopub.status.idle": "2023-02-05T20:01:13.894903Z",
          "shell.execute_reply": "2023-02-05T20:01:13.893993Z"
        },
        "id": "9af87479",
        "papermill": {
          "duration": 0.046505,
          "end_time": "2023-02-05T20:01:13.896831",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.850326",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=3.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%1000==0:\n",
        "              console.print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a19185",
      "metadata": {
        "id": "83a19185",
        "papermill": {
          "duration": 0.032883,
          "end_time": "2023-02-05T20:01:13.962889",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.930006",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed83761a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:14.031028Z",
          "iopub.status.busy": "2023-02-05T20:01:14.030673Z",
          "iopub.status.idle": "2023-02-05T20:01:14.045721Z",
          "shell.execute_reply": "2023-02-05T20:01:14.044853Z"
        },
        "id": "ed83761a",
        "papermill": {
          "duration": 0.05186,
          "end_time": "2023-02-05T20:01:14.047755",
          "exception": false,
          "start_time": "2023-02-05T20:01:13.995895",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
        "def T5Trainer(trainnews, valnews, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
        "  \n",
        "  \"\"\"\n",
        "  T5 trainer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Set random seeds and deterministic pytorch for reproducibility\n",
        "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
        "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  # logging\n",
        "  console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "  model = model.to(device)\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "  # Importing the raw dataset\n",
        "  #dataframe = dataframe[[source_text,target_text]]\n",
        "  trainnews= trainnews[[source_text,target_text]]\n",
        "  valnews=valnews[[source_text,target_text]]\n",
        "  #display_df(dataframe.head(2))\n",
        "  display_df(trainnews.head(2))\n",
        "  display_df(valnews.head(2))\n",
        "\n",
        "  \n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "  #train_size = 0.8\n",
        "  #train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
        "  #val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "  #train_dataset = train_dataset.reset_index(drop=True)\n",
        "  train_dataset = trainnews.reset_index(drop=True)\n",
        "  val_dataset = valnews.reset_index(drop=True)\n",
        "\n",
        "\n",
        "  #console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "  #console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  #console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "  console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "  training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  \n",
        "\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "  training_loader = DataLoader(training_set, **train_params)\n",
        "  val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "\n",
        "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  console.log(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "        #pass\n",
        "      \n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  #path = os.path.join(output_dir, \"model_files\")\n",
        "  #model.save_pretrained(path)\n",
        "  model.save_pretrained(output_dir)\n",
        "  tokenizer.save_pretrained(output_dir)\n",
        "  #model_dir=\"./model_files/\"\n",
        "  #model_dir=\".\"  \n",
        "  #tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "  #model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "  model.cuda()\n",
        "\n",
        "\n",
        "  # evaluating test dataset\n",
        "  console.log(f\"[Initiating Validation]...\\n\")\n",
        "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  \n",
        "  console.save_text(os.path.join(output_dir,'logs.txt'))\n",
        "  \n",
        "  console.log(f\"[Validation Completed.]\\n\")\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2349bad2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:14.115617Z",
          "iopub.status.busy": "2023-02-05T20:01:14.114719Z",
          "iopub.status.idle": "2023-02-05T20:01:14.120378Z",
          "shell.execute_reply": "2023-02-05T20:01:14.119532Z"
        },
        "id": "2349bad2",
        "papermill": {
          "duration": 0.041779,
          "end_time": "2023-02-05T20:01:14.122400",
          "exception": false,
          "start_time": "2023-02-05T20:01:14.080621",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_params={\n",
        "    \"MODEL\" : \"t5-small\",\n",
        "    \"TRAIN_BATCH_SIZE\":16,          # training batch size\n",
        "    \"VALID_BATCH_SIZE\":16,          # validation batch size\n",
        "    \"TRAIN_EPOCHS\":5,              # number of training epochs\n",
        "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
        "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\":64,   # max length of target text\n",
        "    \"SEED\": 42                     # set seed for reproducibility \n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e845dc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-05T20:01:14.190162Z",
          "iopub.status.busy": "2023-02-05T20:01:14.189279Z",
          "iopub.status.idle": "2023-02-06T02:18:22.742967Z",
          "shell.execute_reply": "2023-02-06T02:18:22.742041Z"
        },
        "id": "59e845dc",
        "outputId": "4ad67983-889c-4028-8898-162517341810",
        "papermill": {
          "duration": 22628.629654,
          "end_time": "2023-02-06T02:18:22.785116",
          "exception": false,
          "start_time": "2023-02-05T20:01:14.155462",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "2c1d34f5c0684b1994b56bf94e8f7842",
            "e5528315a5c842248daab86cf67ab1fd",
            "ec592eb708074c17bc7e307500435564"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:01:14] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-small<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                            <a href=\"file:///tmp/ipykernel_23/3469013183.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469013183.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_23/3469013183.py#15\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[20:01:14]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-small\u001b[33m...\u001b[0m                                                            \u001b]8;id=927228;file:///tmp/ipykernel_23/3469013183.py\u001b\\\u001b[2m3469013183.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=897924;file:///tmp/ipykernel_23/3469013183.py#15\u001b\\\u001b[2m15\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c1d34f5c0684b1994b56bf94e8f7842",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5528315a5c842248daab86cf67ab1fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec592eb708074c17bc7e307500435564",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[20:01:29] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                 <a href=\"file:///tmp/ipykernel_23/3469013183.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469013183.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_23/3469013183.py#26\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[20:01:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                                                 \u001b]8;id=852529;file:///tmp/ipykernel_23/3469013183.py\u001b\\\u001b[2m3469013183.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=423422;file:///tmp/ipykernel_23/3469013183.py#26\u001b\\\u001b[2m26\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">                      source_text                       </span>|<span style=\"font-weight: bold\">                       target_text                      </span>|\n",
              "|--------------------------------------------------------+--------------------------------------------------------|\n",
              "| summarize: By . Associated Press . PUBLISHED: . 14:11  | Bishop John Folda, of North Dakota, is taking time off |\n",
              "|  EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25   |                 after being diagnosed .                |\n",
              "|October 2013 . The bishop of the Fargo Catholic Diocese |  He contracted the infection through contaminated food |\n",
              "|  in North Dakota has exposed potentially hundreds of   |                       in Italy .                       |\n",
              "| church members in Fargo, Grand Forks and Jamestown to  |   Church members in Fargo, Grand Forks and Jamestown   |\n",
              "|   the hepatitis A virus in late September and early    |                could have been exposed .               |\n",
              "|  October. The state Health Department has issued an    |                                                        |\n",
              "|   advisory of exposure for anyone who attended five    |                                                        |\n",
              "|    churches and took communion. Bishop John Folda      |                                                        |\n",
              "|   (pictured) of the Fargo Catholic Diocese in North    |                                                        |\n",
              "|   Dakota has exposed potentially hundreds of church    |                                                        |\n",
              "|  members in Fargo, Grand Forks and Jamestown to the    |                                                        |\n",
              "|hepatitis A . State Immunization Program Manager Molly  |                                                        |\n",
              "| Howell says the risk is low, but officials feel it's   |                                                        |\n",
              "|important to alert people to the possible exposure. The |                                                        |\n",
              "| diocese announced on Monday that Bishop John Folda is  |                                                        |\n",
              "|taking time off after being diagnosed with hepatitis A. |                                                        |\n",
              "| The diocese says he contracted the infection through   |                                                        |\n",
              "|  contaminated food while attending a conference for    |                                                        |\n",
              "|newly ordained bishops in Italy last month. Symptoms of |                                                        |\n",
              "|hepatitis A include fever, tiredness, loss of appetite, |                                                        |\n",
              "|nausea and abdominal discomfort. Fargo Catholic Diocese |                                                        |\n",
              "|   in North Dakota (pictured) is where the bishop is    |                                                        |\n",
              "|                       located .                        |                                                        |\n",
              "|     summarize: Ralph Mata was an internal affairs      |  Criminal complaint: Cop used his role to help cocaine |\n",
              "|   lieutenant for the Miami-Dade Police Department,     |                      traffickers .                     |\n",
              "| working in the division that investigates allegations  |  Ralph Mata, an internal affairs lieutenant, allegedly |\n",
              "|of wrongdoing by cops. Outside the office, authorities  |                 helped group get guns .                |\n",
              "|  allege that the 45-year-old longtime officer worked   | He also arranged to pay two assassins in a murder plot,|\n",
              "|  with a drug trafficking organization to help plan a   |                  a complaint alleges .                 |\n",
              "|murder plot and get guns. A criminal complaint unsealed |                                                        |\n",
              "| in U.S. District Court in New Jersey Tuesday accuses   |                                                        |\n",
              "| Mata, also known as \"The Milk Man,\" of using his role  |                                                        |\n",
              "|   as a police officer to help the drug trafficking     |                                                        |\n",
              "|organization in exchange for money and gifts, including |                                                        |\n",
              "|a Rolex watch. In one instance, the complaint alleges,  |                                                        |\n",
              "| Mata arranged to pay two assassins to kill rival drug  |                                                        |\n",
              "| dealers. The killers would pose as cops, pulling over  |                                                        |\n",
              "| their targets before shooting them, according to the   |                                                        |\n",
              "|complaint. \"Ultimately, the (organization) decided not  |                                                        |\n",
              "| to move forward with the murder plot, but Mata still   |                                                        |\n",
              "|   received a payment for setting up the meetings,\"     |                                                        |\n",
              "|federal prosecutors said in a statement. The complaint  |                                                        |\n",
              "|    also alleges that Mata used his police badge to     |                                                        |\n",
              "|purchase weapons for drug traffickers. Mata, according  |                                                        |\n",
              "|to the complaint, then used contacts at the airport to  |                                                        |\n",
              "|transport the weapons in his carry-on luggage on trips  |                                                        |\n",
              "| from Miami to the Dominican Republic. Court documents  |                                                        |\n",
              "| released by investigators do not specify the name of   |                                                        |\n",
              "|   the drug trafficking organization with which Mata    |                                                        |\n",
              "|allegedly conspired but says the organization has been  |                                                        |\n",
              "|importing narcotics from places such as Ecuador and the |                                                        |\n",
              "|  Dominican Republic by hiding them \"inside shipping    |                                                        |\n",
              "|  containers containing pallets of produce, including   |                                                        |\n",
              "|   bananas.\" The organization \"has been distributing    |                                                        |\n",
              "| narcotics in New Jersey and elsewhere,\" the complaint  |                                                        |\n",
              "|  says. Authorities arrested Mata on Tuesday in Miami   |                                                        |\n",
              "|Gardens, Florida. It was not immediately clear whether  |                                                        |\n",
              "|Mata has an attorney, and police officials could not be |                                                        |\n",
              "| immediately reached for comment. Mata has worked for   |                                                        |\n",
              "|the Miami-Dade Police Department since 1992, including  |                                                        |\n",
              "| directing investigations in Miami Gardens and working  |                                                        |\n",
              "|as a lieutenant in the K-9 unit at Miami International  |                                                        |\n",
              "|Airport, according to the complaint. Since March 2010,  |                                                        |\n",
              "| he had been working in the internal affairs division.  |                                                        |\n",
              "|Mata faces charges of aiding and abetting a conspiracy  |                                                        |\n",
              "|to distribute cocaine, conspiring to distribute cocaine |                                                        |\n",
              "|   and engaging in monetary transactions in property    |                                                        |\n",
              "|    derived from specified unlawful activity. He is     |                                                        |\n",
              "|  scheduled to appear in federal court in Florida on    |                                                        |\n",
              "|   Wednesday. If convicted, Mata could face life in     |                                                        |\n",
              "|   prison. CNN's Suzanne Presto contributed to this     |                                                        |\n",
              "|                        report.                         |                                                        |\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n",
              "|\u001b[1m                      source_text                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                      target_text                      \u001b[0m|\n",
              "|--------------------------------------------------------+--------------------------------------------------------|\n",
              "| summarize: By . Associated Press . PUBLISHED: . 14:11  | Bishop John Folda, of North Dakota, is taking time off |\n",
              "|  EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25   |                 after being diagnosed .                |\n",
              "|October 2013 . The bishop of the Fargo Catholic Diocese |  He contracted the infection through contaminated food |\n",
              "|  in North Dakota has exposed potentially hundreds of   |                       in Italy .                       |\n",
              "| church members in Fargo, Grand Forks and Jamestown to  |   Church members in Fargo, Grand Forks and Jamestown   |\n",
              "|   the hepatitis A virus in late September and early    |                could have been exposed .               |\n",
              "|  October. The state Health Department has issued an    |                                                        |\n",
              "|   advisory of exposure for anyone who attended five    |                                                        |\n",
              "|    churches and took communion. Bishop John Folda      |                                                        |\n",
              "|   (pictured) of the Fargo Catholic Diocese in North    |                                                        |\n",
              "|   Dakota has exposed potentially hundreds of church    |                                                        |\n",
              "|  members in Fargo, Grand Forks and Jamestown to the    |                                                        |\n",
              "|hepatitis A . State Immunization Program Manager Molly  |                                                        |\n",
              "| Howell says the risk is low, but officials feel it's   |                                                        |\n",
              "|important to alert people to the possible exposure. The |                                                        |\n",
              "| diocese announced on Monday that Bishop John Folda is  |                                                        |\n",
              "|taking time off after being diagnosed with hepatitis A. |                                                        |\n",
              "| The diocese says he contracted the infection through   |                                                        |\n",
              "|  contaminated food while attending a conference for    |                                                        |\n",
              "|newly ordained bishops in Italy last month. Symptoms of |                                                        |\n",
              "|hepatitis A include fever, tiredness, loss of appetite, |                                                        |\n",
              "|nausea and abdominal discomfort. Fargo Catholic Diocese |                                                        |\n",
              "|   in North Dakota (pictured) is where the bishop is    |                                                        |\n",
              "|                       located .                        |                                                        |\n",
              "|     summarize: Ralph Mata was an internal affairs      |  Criminal complaint: Cop used his role to help cocaine |\n",
              "|   lieutenant for the Miami-Dade Police Department,     |                      traffickers .                     |\n",
              "| working in the division that investigates allegations  |  Ralph Mata, an internal affairs lieutenant, allegedly |\n",
              "|of wrongdoing by cops. Outside the office, authorities  |                 helped group get guns .                |\n",
              "|  allege that the 45-year-old longtime officer worked   | He also arranged to pay two assassins in a murder plot,|\n",
              "|  with a drug trafficking organization to help plan a   |                  a complaint alleges .                 |\n",
              "|murder plot and get guns. A criminal complaint unsealed |                                                        |\n",
              "| in U.S. District Court in New Jersey Tuesday accuses   |                                                        |\n",
              "| Mata, also known as \"The Milk Man,\" of using his role  |                                                        |\n",
              "|   as a police officer to help the drug trafficking     |                                                        |\n",
              "|organization in exchange for money and gifts, including |                                                        |\n",
              "|a Rolex watch. In one instance, the complaint alleges,  |                                                        |\n",
              "| Mata arranged to pay two assassins to kill rival drug  |                                                        |\n",
              "| dealers. The killers would pose as cops, pulling over  |                                                        |\n",
              "| their targets before shooting them, according to the   |                                                        |\n",
              "|complaint. \"Ultimately, the (organization) decided not  |                                                        |\n",
              "| to move forward with the murder plot, but Mata still   |                                                        |\n",
              "|   received a payment for setting up the meetings,\"     |                                                        |\n",
              "|federal prosecutors said in a statement. The complaint  |                                                        |\n",
              "|    also alleges that Mata used his police badge to     |                                                        |\n",
              "|purchase weapons for drug traffickers. Mata, according  |                                                        |\n",
              "|to the complaint, then used contacts at the airport to  |                                                        |\n",
              "|transport the weapons in his carry-on luggage on trips  |                                                        |\n",
              "| from Miami to the Dominican Republic. Court documents  |                                                        |\n",
              "| released by investigators do not specify the name of   |                                                        |\n",
              "|   the drug trafficking organization with which Mata    |                                                        |\n",
              "|allegedly conspired but says the organization has been  |                                                        |\n",
              "|importing narcotics from places such as Ecuador and the |                                                        |\n",
              "|  Dominican Republic by hiding them \"inside shipping    |                                                        |\n",
              "|  containers containing pallets of produce, including   |                                                        |\n",
              "|   bananas.\" The organization \"has been distributing    |                                                        |\n",
              "| narcotics in New Jersey and elsewhere,\" the complaint  |                                                        |\n",
              "|  says. Authorities arrested Mata on Tuesday in Miami   |                                                        |\n",
              "|Gardens, Florida. It was not immediately clear whether  |                                                        |\n",
              "|Mata has an attorney, and police officials could not be |                                                        |\n",
              "| immediately reached for comment. Mata has worked for   |                                                        |\n",
              "|the Miami-Dade Police Department since 1992, including  |                                                        |\n",
              "| directing investigations in Miami Gardens and working  |                                                        |\n",
              "|as a lieutenant in the K-9 unit at Miami International  |                                                        |\n",
              "|Airport, according to the complaint. Since March 2010,  |                                                        |\n",
              "| he had been working in the internal affairs division.  |                                                        |\n",
              "|Mata faces charges of aiding and abetting a conspiracy  |                                                        |\n",
              "|to distribute cocaine, conspiring to distribute cocaine |                                                        |\n",
              "|   and engaging in monetary transactions in property    |                                                        |\n",
              "|    derived from specified unlawful activity. He is     |                                                        |\n",
              "|  scheduled to appear in federal court in Florida on    |                                                        |\n",
              "|   Wednesday. If convicted, Mata could face life in     |                                                        |\n",
              "|   prison. CNN's Suzanne Presto contributed to this     |                                                        |\n",
              "|                        report.                         |                                                        |\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Sample Data                                                    </span>\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">                      source_text                       </span>|<span style=\"font-weight: bold\">                       target_text                      </span>|\n",
              "|--------------------------------------------------------+--------------------------------------------------------|\n",
              "|summarize: Sally Forrest, an actress-dancer who graced  | Sally Forrest, an actress-dancer who graced the silver |\n",
              "| the silver screen throughout the '40s and '50s in MGM  | screen throughout the '40s and '50s in MGM musicals and|\n",
              "|musicals and films such as the 1956 noir While the City |                films died on March 15 .                |\n",
              "| Sleeps died on March 15 at her home in Beverly Hills,  |   Forrest, whose birth name was Katherine Feeney, had  |\n",
              "|  California. Forrest, whose birth name was Katherine   |                  long battled cancer .                 |\n",
              "|    Feeney, was 86 and had long battled cancer. Her     |     A San Diego native, Forrest became a protege of    |\n",
              "|publicist, Judith Goffin, announced the news Thursday.  |    Hollywood trailblazer Ida Lupino, who cast her in   |\n",
              "| Scroll down for video . Actress: Sally Forrest was in  |                starring roles in films .               |\n",
              "|   the 1951 Ida Lupino-directed film 'Hard, Fast and    |                                                        |\n",
              "|Beautiful' (left) and the 1956 Fritz Lang movie 'While  |                                                        |\n",
              "| the City Sleeps' A San Diego native, Forrest became a  |                                                        |\n",
              "| protege of Hollywood trailblazer Ida Lupino, who cast  |                                                        |\n",
              "| her in starring roles in films including the critical  |                                                        |\n",
              "|and commercial success Not Wanted, Never Fear and Hard, |                                                        |\n",
              "|   Fast and Beautiful. Some of Forrest's other film     |                                                        |\n",
              "|credits included Bannerline, Son of Sinbad, and Excuse  |                                                        |\n",
              "|  My Dust, according to her iMDB page. The page also    |                                                        |\n",
              "| indicates Forrest was in multiple Climax! and Rawhide  |                                                        |\n",
              "|television episodes. Forrest appeared as herself in an  |                                                        |\n",
              "| episode of The Ed Sullivan Show and three episodes of  |                                                        |\n",
              "|  The Dinah Shore Chevy Show, her iMDB page says. She   |                                                        |\n",
              "|also starred in a Broadway production of The Seven Year |                                                        |\n",
              "|   Itch. City News Service reported that other stage    |                                                        |\n",
              "| credits included As You Like It, No, No, Nanette and   |                                                        |\n",
              "|  Damn Yankees. Forrest married writer-producer Milo    |                                                        |\n",
              "|Frank in 1951. He died in 2004. She is survived by her  |                                                        |\n",
              "|  niece, Sharon Durham, and nephews, Michael and Mark   |                                                        |\n",
              "| Feeney. Career: A San Diego native, Forrest became a   |                                                        |\n",
              "| protege of Hollywood trailblazer Ida Lupino, who cast  |                                                        |\n",
              "|           her in starring roles in films .             |                                                        |\n",
              "| summarize: A middle-school teacher in China has inked  |    Works include pictures of Presidential Palace and   |\n",
              "|hundreds of sketches that are beyond be-leaf. Politics  |                 Yangtze River Bridge .                 |\n",
              "| teacher Wang Lian, 35,  has created 1000 stunning ink  |   Has inked 1,000 pieces of art on leaves in last two  |\n",
              "|    drawings covering subjects as varied as cartoon     |                         years .                        |\n",
              "| characters and landscapes to animals, birds according  |  Gives work away to students in form of bookmarks and  |\n",
              "| to the People's Daily Online. The intricate scribbles  |                       postcards .                      |\n",
              "|  on leaves feature Wang's favourite sites across the   |                                                        |\n",
              "|city of Nanjing, which include the Presidential Palace, |                                                        |\n",
              "|Yangtze River Bridge, the ancient Jiming Temple and the |                                                        |\n",
              "|Qinhuai River. Natural canvas: Artist and teacher Wang  |                                                        |\n",
              "| Lian has done hundreds of drawings, like this temple,  |                                                        |\n",
              "|on leaves she collects in the park and on the streets . |                                                        |\n",
              "|Delicate: She uses an ink pen to gently draw the local  |                                                        |\n",
              "|    scenes and buildings on the dried out leaves .      |                                                        |\n",
              "| 'Although teaching politics is my job, drawing is my   |                                                        |\n",
              "| passion and hobby,' said Wang. 'I first tried drawing  |                                                        |\n",
              "| on leaves about 10 years ago and fell in love with it  |                                                        |\n",
              "|as an art form immediately. 'It's like drawing on very  |                                                        |\n",
              "|old parchment paper, you have to be really careful that |                                                        |\n",
              "| you don't damage the leaf because it is very fragile   |                                                        |\n",
              "|  and this helps focus your attention and abilities.'   |                                                        |\n",
              "|Wang started giving the drawings away on Christmas Eve  |                                                        |\n",
              "|   in 2012 when her junior high school son came home    |                                                        |\n",
              "|    saying he wanted to prepare some gifts for his      |                                                        |\n",
              "|classmates. Being an avid painter, Wang decided to give |                                                        |\n",
              "|   her son's friends unique presents of gingko leaf     |                                                        |\n",
              "|paintings. Wang loves gingko leaves and will often pick |                                                        |\n",
              "|  them up along Gingko Avenue, near to her school, in   |                                                        |\n",
              "|Nanjing in east China's Jiangsu province. Every autumn  |                                                        |\n",
              "|  she collects about 2,000 leaves from the ground to    |                                                        |\n",
              "| ensure she has enough to cover spoils too. Intricate:  |                                                        |\n",
              "|  Teacher Wang has drawn hundreds of local scenes on    |                                                        |\n",
              "|  leaves she has collected from the park . Hobby: The   |                                                        |\n",
              "|artist collects leaves every autumn and dries them out  |                                                        |\n",
              "| so she can sketch these impressive building scenes .   |                                                        |\n",
              "|'The colour and shape of gingko leaves are particularly |                                                        |\n",
              "| beautiful,' she said. 'I need to collect around 2000   |                                                        |\n",
              "|  leaves because this will include losses'. She takes   |                                                        |\n",
              "|them home where she then presses them between the pages |                                                        |\n",
              "| of books. 'Luckily, I have quite a lot of books and I  |                                                        |\n",
              "|try to use old ones or ones that I've already read so I |                                                        |\n",
              "|   don't end up with nothing to read.' Once they are    |                                                        |\n",
              "| dried, she carefully takes each one and using an ink   |                                                        |\n",
              "|fountain pen creates her masterpieces. She said: 'Some  |                                                        |\n",
              "| people are into capturing beauty through photography,  |                                                        |\n",
              "| but for me, a digitalised image just isn't the same.   |                                                        |\n",
              "|New leaf: Politics teacher Wang Lian has drawn hundreds |                                                        |\n",
              "|   of doodles on leaves for the last 10 years . 'By     |                                                        |\n",
              "|  drawing what I see I become far more a part of the    |                                                        |\n",
              "|process and part of the final piece. 'One day I hope to |                                                        |\n",
              "| be able to put my collection on display, but for now   |                                                        |\n",
              "|  it's really just for my own pleasure.' Wang's leaf    |                                                        |\n",
              "|  paintings are turned into bookmarks, postcards and    |                                                        |\n",
              "| sometimes even given as gifts to her her students so   |                                                        |\n",
              "|she can share the beauty of leaf paintings. But locals  |                                                        |\n",
              "| who have had the luck of being able to see Wang's art  |                                                        |\n",
              "| have been gobsmacked. Local art collector On Hao, 58,  |                                                        |\n",
              "|    said: 'These are truly remarkable and beautiful     |                                                        |\n",
              "|  creations. 'She has so much talent she is wasted in   |                                                        |\n",
              "|                      teaching.'                        |                                                        |\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                                                    Sample Data                                                    \u001b[0m\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n",
              "|\u001b[1m                      source_text                      \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                      target_text                      \u001b[0m|\n",
              "|--------------------------------------------------------+--------------------------------------------------------|\n",
              "|summarize: Sally Forrest, an actress-dancer who graced  | Sally Forrest, an actress-dancer who graced the silver |\n",
              "| the silver screen throughout the '40s and '50s in MGM  | screen throughout the '40s and '50s in MGM musicals and|\n",
              "|musicals and films such as the 1956 noir While the City |                films died on March 15 .                |\n",
              "| Sleeps died on March 15 at her home in Beverly Hills,  |   Forrest, whose birth name was Katherine Feeney, had  |\n",
              "|  California. Forrest, whose birth name was Katherine   |                  long battled cancer .                 |\n",
              "|    Feeney, was 86 and had long battled cancer. Her     |     A San Diego native, Forrest became a protege of    |\n",
              "|publicist, Judith Goffin, announced the news Thursday.  |    Hollywood trailblazer Ida Lupino, who cast her in   |\n",
              "| Scroll down for video . Actress: Sally Forrest was in  |                starring roles in films .               |\n",
              "|   the 1951 Ida Lupino-directed film 'Hard, Fast and    |                                                        |\n",
              "|Beautiful' (left) and the 1956 Fritz Lang movie 'While  |                                                        |\n",
              "| the City Sleeps' A San Diego native, Forrest became a  |                                                        |\n",
              "| protege of Hollywood trailblazer Ida Lupino, who cast  |                                                        |\n",
              "| her in starring roles in films including the critical  |                                                        |\n",
              "|and commercial success Not Wanted, Never Fear and Hard, |                                                        |\n",
              "|   Fast and Beautiful. Some of Forrest's other film     |                                                        |\n",
              "|credits included Bannerline, Son of Sinbad, and Excuse  |                                                        |\n",
              "|  My Dust, according to her iMDB page. The page also    |                                                        |\n",
              "| indicates Forrest was in multiple Climax! and Rawhide  |                                                        |\n",
              "|television episodes. Forrest appeared as herself in an  |                                                        |\n",
              "| episode of The Ed Sullivan Show and three episodes of  |                                                        |\n",
              "|  The Dinah Shore Chevy Show, her iMDB page says. She   |                                                        |\n",
              "|also starred in a Broadway production of The Seven Year |                                                        |\n",
              "|   Itch. City News Service reported that other stage    |                                                        |\n",
              "| credits included As You Like It, No, No, Nanette and   |                                                        |\n",
              "|  Damn Yankees. Forrest married writer-producer Milo    |                                                        |\n",
              "|Frank in 1951. He died in 2004. She is survived by her  |                                                        |\n",
              "|  niece, Sharon Durham, and nephews, Michael and Mark   |                                                        |\n",
              "| Feeney. Career: A San Diego native, Forrest became a   |                                                        |\n",
              "| protege of Hollywood trailblazer Ida Lupino, who cast  |                                                        |\n",
              "|           her in starring roles in films .             |                                                        |\n",
              "| summarize: A middle-school teacher in China has inked  |    Works include pictures of Presidential Palace and   |\n",
              "|hundreds of sketches that are beyond be-leaf. Politics  |                 Yangtze River Bridge .                 |\n",
              "| teacher Wang Lian, 35,  has created 1000 stunning ink  |   Has inked 1,000 pieces of art on leaves in last two  |\n",
              "|    drawings covering subjects as varied as cartoon     |                         years .                        |\n",
              "| characters and landscapes to animals, birds according  |  Gives work away to students in form of bookmarks and  |\n",
              "| to the People's Daily Online. The intricate scribbles  |                       postcards .                      |\n",
              "|  on leaves feature Wang's favourite sites across the   |                                                        |\n",
              "|city of Nanjing, which include the Presidential Palace, |                                                        |\n",
              "|Yangtze River Bridge, the ancient Jiming Temple and the |                                                        |\n",
              "|Qinhuai River. Natural canvas: Artist and teacher Wang  |                                                        |\n",
              "| Lian has done hundreds of drawings, like this temple,  |                                                        |\n",
              "|on leaves she collects in the park and on the streets . |                                                        |\n",
              "|Delicate: She uses an ink pen to gently draw the local  |                                                        |\n",
              "|    scenes and buildings on the dried out leaves .      |                                                        |\n",
              "| 'Although teaching politics is my job, drawing is my   |                                                        |\n",
              "| passion and hobby,' said Wang. 'I first tried drawing  |                                                        |\n",
              "| on leaves about 10 years ago and fell in love with it  |                                                        |\n",
              "|as an art form immediately. 'It's like drawing on very  |                                                        |\n",
              "|old parchment paper, you have to be really careful that |                                                        |\n",
              "| you don't damage the leaf because it is very fragile   |                                                        |\n",
              "|  and this helps focus your attention and abilities.'   |                                                        |\n",
              "|Wang started giving the drawings away on Christmas Eve  |                                                        |\n",
              "|   in 2012 when her junior high school son came home    |                                                        |\n",
              "|    saying he wanted to prepare some gifts for his      |                                                        |\n",
              "|classmates. Being an avid painter, Wang decided to give |                                                        |\n",
              "|   her son's friends unique presents of gingko leaf     |                                                        |\n",
              "|paintings. Wang loves gingko leaves and will often pick |                                                        |\n",
              "|  them up along Gingko Avenue, near to her school, in   |                                                        |\n",
              "|Nanjing in east China's Jiangsu province. Every autumn  |                                                        |\n",
              "|  she collects about 2,000 leaves from the ground to    |                                                        |\n",
              "| ensure she has enough to cover spoils too. Intricate:  |                                                        |\n",
              "|  Teacher Wang has drawn hundreds of local scenes on    |                                                        |\n",
              "|  leaves she has collected from the park . Hobby: The   |                                                        |\n",
              "|artist collects leaves every autumn and dries them out  |                                                        |\n",
              "| so she can sketch these impressive building scenes .   |                                                        |\n",
              "|'The colour and shape of gingko leaves are particularly |                                                        |\n",
              "| beautiful,' she said. 'I need to collect around 2000   |                                                        |\n",
              "|  leaves because this will include losses'. She takes   |                                                        |\n",
              "|them home where she then presses them between the pages |                                                        |\n",
              "| of books. 'Luckily, I have quite a lot of books and I  |                                                        |\n",
              "|try to use old ones or ones that I've already read so I |                                                        |\n",
              "|   don't end up with nothing to read.' Once they are    |                                                        |\n",
              "| dried, she carefully takes each one and using an ink   |                                                        |\n",
              "|fountain pen creates her masterpieces. She said: 'Some  |                                                        |\n",
              "| people are into capturing beauty through photography,  |                                                        |\n",
              "| but for me, a digitalised image just isn't the same.   |                                                        |\n",
              "|New leaf: Politics teacher Wang Lian has drawn hundreds |                                                        |\n",
              "|   of doodles on leaves for the last 10 years . 'By     |                                                        |\n",
              "|  drawing what I see I become far more a part of the    |                                                        |\n",
              "|process and part of the final piece. 'One day I hope to |                                                        |\n",
              "| be able to put my collection on display, but for now   |                                                        |\n",
              "|  it's really just for my own pleasure.' Wang's leaf    |                                                        |\n",
              "|  paintings are turned into bookmarks, postcards and    |                                                        |\n",
              "| sometimes even given as gifts to her her students so   |                                                        |\n",
              "|she can share the beauty of leaf paintings. But locals  |                                                        |\n",
              "| who have had the luck of being able to see Wang's art  |                                                        |\n",
              "| have been gobsmacked. Local art collector On Hao, 58,  |                                                        |\n",
              "|    said: 'These are truly remarkable and beautiful     |                                                        |\n",
              "|  creations. 'She has so much talent she is wasted in   |                                                        |\n",
              "|                      teaching.'                        |                                                        |\n",
              "+-----------------------------------------------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100000</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m100000\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13368</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m13368\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                             <a href=\"file:///tmp/ipykernel_23/3469013183.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469013183.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_23/3469013183.py#86\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">86</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                             \u001b]8;id=663062;file:///tmp/ipykernel_23/3469013183.py\u001b\\\u001b[2m3469013183.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=385189;file:///tmp/ipykernel_23/3469013183.py#86\u001b\\\u001b[2m86\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 4000  | tensor(1.7529, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 4000  | tensor(1.7529, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 4000  | tensor(1.7529, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 5000  | tensor(1.9984, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 4000  | tensor(1.7529, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 5000  | tensor(1.9984, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                               Training Status                               </span>\n",
              "+---------------------------------------------------------------------------+\n",
              "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                            </span>|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 4000  | tensor(1.7529, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 5000  | tensor(1.9984, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "|  4   | 6000  | tensor(2.1256, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;)|\n",
              "+---------------------------------------------------------------------------+\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3m                               Training Status                               \u001b[0m\n",
              "+---------------------------------------------------------------------------+\n",
              "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                            \u001b[0m|\n",
              "|------+-------+------------------------------------------------------------|\n",
              "|  0   |   0   | tensor(3.2006, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 1000  | tensor(2.0326, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 2000  | tensor(2.0432, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 3000  | tensor(1.9783, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 4000  | tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 5000  | tensor(2.1397, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  0   | 6000  | tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   |   0   | tensor(2.3029, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 1000  | tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 2000  | tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 3000  | tensor(1.9489, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 4000  | tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 5000  | tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  1   | 6000  | tensor(1.9725, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   |   0   | tensor(1.9573, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 1000  | tensor(2.1012, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 2000  | tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 3000  | tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 4000  | tensor(1.5952, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 5000  | tensor(2.0667, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  2   | 6000  | tensor(2.3707, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   |   0   | tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 1000  | tensor(1.7697, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 2000  | tensor(1.8407, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 3000  | tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 4000  | tensor(2.2617, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 5000  | tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  3   | 6000  | tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   |   0   | tensor(1.7682, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 1000  | tensor(1.9393, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 2000  | tensor(2.0345, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 3000  | tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 4000  | tensor(1.7529, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 5000  | tensor(1.9984, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "|  4   | 6000  | tensor(2.1256, device='cuda:0', grad_fn=<NllLossBackward0>)|\n",
              "+---------------------------------------------------------------------------+\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[01:40:05] </span><span style=\"font-weight: bold\">[</span>Saving Model<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                       <a href=\"file:///tmp/ipykernel_23/3469013183.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469013183.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_23/3469013183.py#92\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">92</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[01:40:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mSaving Model\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                                       \u001b]8;id=190729;file:///tmp/ipykernel_23/3469013183.py\u001b\\\u001b[2m3469013183.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=872827;file:///tmp/ipykernel_23/3469013183.py#92\u001b\\\u001b[2m92\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                                        \u001b[2m                \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"font-weight: bold\">[</span>Initiating Validation<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                             <a href=\"file:///tmp/ipykernel_23/3469013183.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469013183.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_23/3469013183.py#106\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Validation\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                                             \u001b]8;id=720405;file:///tmp/ipykernel_23/3469013183.py\u001b\\\u001b[2m3469013183.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=916405;file:///tmp/ipykernel_23/3469013183.py#106\u001b\\\u001b[2m106\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                                       \u001b[2m                 \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Completed <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Completed \u001b[1;36m0\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02:18:22] </span><span style=\"font-weight: bold\">[</span>Validation Completed.<span style=\"font-weight: bold\">]</span>                                                                <a href=\"file:///tmp/ipykernel_23/3469013183.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3469013183.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_23/3469013183.py#114\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[02:18:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mValidation Completed.\u001b[1m]\u001b[0m                                                                \u001b]8;id=136275;file:///tmp/ipykernel_23/3469013183.py\u001b\\\u001b[2m3469013183.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=247740;file:///tmp/ipykernel_23/3469013183.py#114\u001b\\\u001b[2m114\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m                                                                                       \u001b[2m                 \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span> Model saved @ .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">model_files</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m Model saved @ .\u001b[35m/\u001b[0m\u001b[95mmodel_files\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Validation<span style=\"font-weight: bold\">]</span> Generation on Validation data saved @ .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">predictions.csv</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0mValidation\u001b[1m]\u001b[0m Generation on Validation data saved @ .\u001b[35m/\u001b[0m\u001b[95mpredictions.csv\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Logs<span style=\"font-weight: bold\">]</span> Logs saved @ .<span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">logs.txt</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m[\u001b[0mLogs\u001b[1m]\u001b[0m Logs saved @ .\u001b[35m/\u001b[0m\u001b[95mlogs.txt\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "T5Trainer(trainnews, valnews, source_text=\"article\", target_text=\"highlights\", model_params=model_params, output_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeaf753a",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-29T10:00:27.281249Z",
          "iopub.status.idle": "2023-01-29T10:00:27.281762Z",
          "shell.execute_reply": "2023-01-29T10:00:27.281532Z",
          "shell.execute_reply.started": "2023-01-29T10:00:27.281508Z"
        },
        "id": "eeaf753a",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_dir=\".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab9e3c6",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-29T10:00:27.283282Z",
          "iopub.status.idle": "2023-01-29T10:00:27.285382Z",
          "shell.execute_reply": "2023-01-29T10:00:27.285138Z",
          "shell.execute_reply.started": "2023-01-29T10:00:27.285113Z"
        },
        "id": "0ab9e3c6",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(model_dir)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "model.cuda()\n",
        "\n",
        "#T5Tokenizer, T5ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06eaa775",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-29T10:00:27.286875Z",
          "iopub.status.idle": "2023-01-29T10:00:27.287663Z",
          "shell.execute_reply": "2023-01-29T10:00:27.287436Z",
          "shell.execute_reply.started": "2023-01-29T10:00:27.287411Z"
        },
        "id": "06eaa775",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "source_text=\"article\"\n",
        "target_text='highlights'\n",
        "trainnews= trainnews[[source_text,target_text]]\n",
        "valnews=valnews[[source_text,target_text]]\n",
        "  #display_df(dataframe.head(2))\n",
        "display_df(trainnews.head(2))\n",
        "display_df(valnews.head(2))\n",
        "\n",
        "  \n",
        "# Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "  #train_size = 0.8\n",
        "  #train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
        "  #val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "  #train_dataset = train_dataset.reset_index(drop=True)\n",
        "train_dataset = trainnews.reset_index(drop=True)\n",
        "val_dataset = valnews.reset_index(drop=True)\n",
        "\n",
        "\n",
        "  #console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "  #console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  #console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  \n",
        "\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "output_dir='./outputs/'\n",
        "for epoch in range(1):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  \n",
        "console.save_text(os.path.join(output_dir,'logs.txt'))\n",
        "  \n",
        "console.log(f\"[Validation Completed.]\\n\")\n",
        "console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5833cf5",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-29T10:00:27.289105Z",
          "iopub.status.idle": "2023-01-29T10:00:27.289900Z",
          "shell.execute_reply": "2023-01-29T10:00:27.289666Z",
          "shell.execute_reply.started": "2023-01-29T10:00:27.289635Z"
        },
        "id": "f5833cf5",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f722c9f",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-01-29T10:00:27.291297Z",
          "iopub.status.idle": "2023-01-29T10:00:27.292042Z",
          "shell.execute_reply": "2023-01-29T10:00:27.291807Z",
          "shell.execute_reply.started": "2023-01-29T10:00:27.291783Z"
        },
        "id": "0f722c9f",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "final_df.to_csv(os.path.join('./outputs/','predictions.csv'))\n",
        "  \n",
        "console.save_text(os.path.join('./outputs/','logs.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb234733",
      "metadata": {
        "id": "fb234733",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 22794.664971,
      "end_time": "2023-02-06T02:18:26.807598",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-02-05T19:58:32.142627",
      "version": "2.3.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}